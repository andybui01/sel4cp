#
# Copyright 2021, Breakaway Consulting Pty. Ltd.
#
# SPDX-License-Identifier: BSD-2-Clause
#
"""
The purpose of this script is to take as input a system description XML file
and generate a system image suitable for loading by the platform bootloader.

The loader image in the current script is assumed to be a flat binary image
that is directly loaded into physical memory.

This makes use of the `altloader` (and alternative to the normal ELF loader
bootstrap). `altloader` initialises areas of physical memory as described by
a sequence a `regions`. It then jumps to the seL4 kernel.

The `altloader` passes two the kernel two regions of memory:

1: The initial task
2: A region of 'additional memory'.

TODO - Cleanup:

reporting:
  number of rebuilds required.
  warnings
  list all kernel objects
  name all kernel objects



The following abreviations are used in the source code:

* Capability => cap
* Address => addr
* Physical => phys

"""
import sys
from argparse import ArgumentParser
from pathlib import Path
from dataclasses import dataclass
from struct import pack, Struct
from os import environ
from math import log2, ceil
from sys import argv, executable, stderr

from typing import Dict, List, Optional, Tuple, Union

from sel4coreplat.system import SystemConfig, ProtectionDomainVSpace
from sel4coreplat.elf import ElfFile, SegmentType
from sel4coreplat.util import kb, mb, lsb, msb, round_up, round_down, mask_bits, is_power_of_two, MemoryRegion, UserError
from sel4coreplat.sel4 import (
    Sel4Aarch64Regs,
    Sel4Invocation,
    Sel4AsidPoolAssign,
    Sel4PageUpperDirectoryMap,
    Sel4PageDirectoryMap,
    Sel4PageTableMap,
    Sel4PageMap,
    Sel4TcbSetTimeoutEndpoint,
    Sel4TcbSetSchedParams,
    Sel4TcbSetCSpace,
    Sel4TcbSetSpace,
    Sel4TcbSetIpcBuffer,
    Sel4TcbWriteRegisters,
    Sel4TcbBindNotification,
    Sel4DomainSet,
    Sel4TcbResume,
    Sel4TcbSuspend,
    Sel4CnodeMint,
    Sel4CnodeCopy,
    Sel4UntypedRetype,
    Sel4IrqControlGet,
    Sel4IrqHandlerSetNotification,
    Sel4SchedControlConfigureFlags,
    Sel4SchedContextBind,
    emulate_kernel_boot,
    emulate_kernel_boot_partial,
    UntypedObject,
    KernelConfig,
    KernelBootInfo,
    FIXED_OBJECT_SIZES,
    SEL4_UNTYPED_OBJECT,
    SEL4_CNODE_OBJECT,
    SEL4_SCHEDCONTEXT_OBJECT,
    SEL4_TCB_OBJECT,
    SEL4_REPLY_OBJECT,
    SEL4_ENDPOINT_OBJECT,
    SEL4_NOTIFICATION_OBJECT,
    SEL4_VSPACE_OBJECT,
    SEL4_PAGE_UPPER_DIRECTORY_OBJECT,
    SEL4_PAGE_DIRECTORY_OBJECT,
    SEL4_SMALL_PAGE_OBJECT,
    SEL4_LARGE_PAGE_OBJECT,
    SEL4_PAGE_TABLE_OBJECT,
    SLOT_BITS,
    SLOT_SIZE,
    INIT_NULL_CAP_ADDRESS,
    INIT_TCB_CAP_ADDRESS,
    INIT_CNODE_CAP_ADDRESS,
    INIT_VSPACE_CAP_ADDRESS,
    INIT_ASID_POOL_CAP_ADDRESS,
    IRQ_CONTROL_CAP_ADDRESS,
    DOMAIN_CAP_ADDRESS,
    SEL4_RIGHTS_ALL,
    SEL4_RIGHTS_READ,
    SEL4_RIGHTS_WRITE,
    SEL4_ARM_DEFAULT_VMATTRIBUTES,
    SEL4_ARM_EXECUTE_NEVER,
    SEL4_ARM_PARITY_ENABLED,
    SEL4_ARM_PAGE_CACHEABLE,
    SEL4_LARGE_PAGE_SIZE,
    SEL4_PAGE_TABLE_SIZE,
    SEL4_OBJECT_TYPE_NAMES,
)
from sel4coreplat.sysxml import ProtectionDomain, xml2system, SystemDescription, PlatformDescription
from sel4coreplat.sysxml import SysMap, SysMemoryRegion # This shouldn't be needed here as such
from sel4coreplat.loader import Loader

# This is a workaround for: https://github.com/indygreg/PyOxidizer/issues/307
# Basically, pyoxidizer generates code that results in argv[0] being set to None.
# ArgumentParser() very much relies on a non-None argv[0]!
# This very simple work-around sets it to the package name.
if argv[0] is None:
    argv[0] = executable  # type: ignore


default_platform_description = PlatformDescription(
    page_sizes = (0x1_000, 0x200_000)
)

@dataclass
class SysinitConfig:
    untyped_info_symbol_name: str
    untyped_info_header_struct: Struct
    untyped_info_object_struct: Struct
    bootstrap_invocation_count_symbol_name: str
    bootstrap_invocation_data_symbol_name: str
    system_invocation_count_symbol_name: str

    def max_untyped_objects(self, symbol_size: int) -> int:
        return (symbol_size - self.untyped_info_header_struct.size) // self.untyped_info_object_struct.size

# The sysinit config is fixed (unless the sysinit C code
# changes the definitions of struct, or the name.
# While this is fixed, we dynamically determine the
# size actual data structures at run time where possible
# to allow for minor changes in the C code without requiring
# rework of this tool
SYSINIT_CONFIG = SysinitConfig(
    untyped_info_symbol_name = "untyped_info",
    untyped_info_header_struct = Struct("<QQ"),
    untyped_info_object_struct = Struct("<QQQ"),
    bootstrap_invocation_count_symbol_name = "bootstrap_invocation_count",
    bootstrap_invocation_data_symbol_name = "bootstrap_invocation_data",
    system_invocation_count_symbol_name = "system_invocation_count",
)

# FIXME: put this somewhere common, autogenerated
MAX_CHILD_PDS = 1

# === PD CSpace layout ===
# The first 4 caps will be identical in both the PD
# and thread CSpace. This is to reduce complexity in libsel4cp.
NULL_CAP = 0 # Not used anywhere, but we intentionally leave this alone
ROOT_PD_EP_CAP_IDX = 1 # EP to communicate with the parent PD
VSPACE_CAP_IDX = 2 # Used for cache management, leave unused for now in PDs

REPLY_CAP_IDX = 4
INPUT_CAP_IDX = 5 # Will be either the notification or endpoint cap
SELF_CNODE_CAP_IDX = 6
SCHEDCONTROL_CAP_IDX = 7

BASE_OUTPUT_NTFN_CAP_IDX = 10
BASE_OUTPUT_EP_CAP_IDX = BASE_OUTPUT_NTFN_CAP_IDX + 64
BASE_IRQ_CAP = BASE_OUTPUT_EP_CAP_IDX + 64
BASE_TCB_CAP = BASE_IRQ_CAP + 64
MAX_SYSTEM_INVOCATION_SIZE = mb(128)
PD_CAP_SIZE = 512
PD_CAP_BITS = int(log2(PD_CAP_SIZE))
PD_SCHEDCONTEXT_SIZE = (1 << 8) # Maximum number of refills in a single scheduling context
EMPTY_THREAD_CAP_SIZE = 4
EMPTY_THREAD_CAP_BITS = int(log2(EMPTY_THREAD_CAP_SIZE))

def get_thread_sc_offset(num_threads, thread) -> int:
    """
    Get offset for a thread's SC in a root PD's CSpace.
    """
    ret = BASE_TCB_CAP + num_threads + thread
    assert ret < PD_CAP_SIZE
    return ret

def get_thread_cspace_offset(num_threads, thread):
    """
    Get offset for a thread's CSpace in a root PD's CSpace.
    """
    ret = BASE_TCB_CAP + num_threads * 2 + thread
    assert ret < PD_CAP_SIZE
    return ret

def get_thread_reply_offset(num_threads, thread):
    """
    Get offset for a Reply capability to a thread in a root PD's CSpace.
    """
    ret = BASE_TCB_CAP + num_threads * 3 + thread
    assert ret < PD_CAP_SIZE
    return ret

def get_pd_vspace_offset(num_threads, pd):
    """
    Get offset for a thread's VSpace in a root PD's CSpace.
    """
    ret = BASE_TCB_CAP + num_threads * 4 + pd 
    assert ret < PD_CAP_SIZE
    return ret

# Badge types:
# Bits 63:62
#   b00 - notification
#   b01 - fault
#   b10 - ppc
#   b11 - root ppc
BADGE_TYPE_BIT      = 62
BADGE_TYPE_NTFN     = (0 << BADGE_TYPE_BIT)
BADGE_TYPE_FAULT    = (1 << BADGE_TYPE_BIT)
BADGE_FAULT_ROOT        = (1 << 61)
BADGE_TYPE_PPC      = (2 << BADGE_TYPE_BIT)
BADGE_TYPE_ROOT_PPC = (3 << BADGE_TYPE_BIT)


def mr_page_bytes(mr: SysMemoryRegion) -> int:
    return 0x1000 if mr.page_size is None else mr.page_size


@dataclass(frozen=True)
class KernelAllocation:
    untyped_cap_address: int  # Fixme: possibly this is an object, not an int?
    phys_addr: int
    allocation_order: int


@dataclass
class UntypedAllocator:
    untyped_object: UntypedObject
    allocation_point: int
    allocations: List[KernelAllocation]

    @property
    def base(self) -> int:
        return self.untyped_object.region.base

    @property
    def end(self) -> int:
        return self.untyped_object.region.end

class KernelObjectAllocator:
    """Allocator for kernel objects.

    This tracks the space available in a set of untyped objects.
    On allocation an untyped with sufficient remaining space is
    returned (while updating the internal tracking).

    Within an untyped object this mimics the kernel's allocation
    policy (basically a bump allocator with alignment).

    The only 'choice' this allocator has is which untyped object
    to use. The current algorithm is simply first fit: the first
    untyped that has sufficient space. This is not optimal.

    Note: The allocator does not generate the Retype invocations;
    this must be done with more knowledge (specifically the destination
    cap) which is distinct.

    It is critical that invocations are generated in the same order
    as the allocations are made.

    """
    def __init__(self, kernel_boot_info: KernelBootInfo) -> None:
        self._allocation_idx = 0
        self._untyped = []
        for ut in kernel_boot_info.untyped_objects:
            if ut.is_device:
                # Kernel allocator can only allocate out of normal memory
                # device memory can't be used for kernel objects
                continue
            self._untyped.append(UntypedAllocator(ut, 0, []))

    def alloc(self, size: int, count: int = 1) -> KernelAllocation:
        assert is_power_of_two(size)
        for ut in self._untyped:
            # See if this fits
            start = round_up(ut.base + ut.allocation_point, size)
            if start + (count * size) <= ut.end:
                ut.allocation_point = (start - ut.base) + (count * size)
                self._allocation_idx += 1
                allocation = KernelAllocation(ut.untyped_object.cap, start, self._allocation_idx)
                ut.allocations.append(allocation)
                return allocation

        raise Exception("Can't alloc - no space")


def invocation_to_str(inv: Sel4Invocation, cap_lookup: Dict[int, str]) -> str:
    arg_strs = []
    for nm, val in inv._args:
        if nm in inv._extra_caps:
            val_str = f"0x{val:016x} ({cap_lookup.get(val)})"
            nm = f"{nm} (cap)"
        elif nm == "src_obj":
            # This is a special cap
            val_str = f"0x{val:016x} ({cap_lookup.get(val)})"
            nm = f"{nm} (cap)"
        elif nm == "vaddr":
            val_str = hex(val)
        elif nm == "size_bits":
            if val == 0:
                val_str = f"{val} (N/A)"
            else:
                val_str = f"{val} (0x{1 << val:x})"
        elif nm == "object_type":
            object_size = FIXED_OBJECT_SIZES.get(val)
            object_type_name = SEL4_OBJECT_TYPE_NAMES[val]
            if object_size is None:
                val_str = f"{val} ({object_type_name} - variable size)"
            else:
                val_str = f"{val} ({object_type_name} - 0x{object_size:x})"
        elif nm == "regs":
            regs = vars(inv.regs)
            val_str = ""
            for i, reg in enumerate(regs.items()):
                reg_value = 0 if reg[1] is None else reg[1]
                if i == 0:
                    val_str = f"{reg[0]} : 0x{reg_value:016x}"
                else:
                    val_str += f"\n{' ':30s}{reg[0]} : 0x{reg_value:016x}"
        else:
            val_str = str(val)
        arg_strs.append(f"         {nm:20s} {val_str}")
    if hasattr(inv, "_repeat_count"):
        arg_strs.append(f"      REPEAT: count={inv._repeat_count} {inv._repeat_incr}")
    args = "\n".join(arg_strs)
    return f"{inv._object_type:20s} - {inv._method_name:17s} - 0x{inv._service:016x} ({cap_lookup.get(inv._service)})\n{args}"


def overlaps(range1: Tuple[int, int], range2: Tuple[int, int]) -> bool:
    """Return true if range1 overlaps range2"""
    base1, size1 = range1
    base2, size2 = range2
    if base1 >= base2 + size2:
        # range1 is completely above range2
        return False
    if  base1 + size1 <= base2:
        # range1 is completely below range2
        return False
    # otherwise there is some overlap
    return True



def phys_mem_regions_from_elf(elf: ElfFile, alignment: int) -> List[MemoryRegion]:
    """Determine the physical memory regions for an ELF file with a given
    alignment.

    The returned region shall be extended (if necessary) so that the start
    and end are congruent with the specified alignment (usually a page size).
    """
    assert alignment > 0
    return [
        MemoryRegion(
            round_down(segment.phys_addr, alignment),
            round_up(segment.phys_addr + len(segment.data), alignment)
        )
        for segment in elf.segments
    ]


def phys_mem_region_from_elf(elf: ElfFile, alignment: int) -> MemoryRegion:
    """Determine a single physical memory region for an ELF.

    Works as per phys_mem_regions_from_elf, but checks the ELF has a single
    segment, and returns the region covering the first segment.
    """
    assert alignment > 0
    assert len(elf.segments) == 1
    return phys_mem_regions_from_elf(elf, alignment)[0]


def virt_mem_regions_from_elf(elf: ElfFile, alignment: int) -> List[MemoryRegion]:
    """Determine the virtual memory regions for an ELF file with a given
    alignment.

    The returned region shall be extended (if necessary) so that the start
    and end are congruent with the specified alignment (usually a page size).
    """
    assert alignment > 0
    return [
        MemoryRegion(
            round_down(segment.virt_addr, alignment),
            round_up(segment.virt_addr + len(segment.data), alignment)
        )
        for segment in elf.segments
    ]


def virt_mem_region_from_elf(elf: ElfFile, alignment: int) -> MemoryRegion:
    """Determine a single virtual memory region for an ELF.

    Works as per virt_mem_regions_from_elf, but checks the ELF has a single
    segment, and returns the region covering the first segment.
    """
    assert alignment > 0
    assert len(elf.segments) == 1
    return virt_mem_regions_from_elf(elf, alignment)[0]


class PageOverlap(Exception):
    pass


class FixedUntypedAlloc:
    def __init__(self, ut: UntypedObject) -> None:
        self._ut = ut
        self.watermark = self._ut.base

    def __lt__(self, other: "FixedUntypedAlloc") -> bool:
        return self._ut.region.base < other._ut.region.base

    def __str__(self) -> str:
        return f"FixedUntypedAlloc(self._ut={self._ut}"

    def __contains__(self, address: int) -> bool:
        return self._ut.region.base <= address < self._ut.region.end

@dataclass(frozen=True, eq=True)
class KernelObject:
    """Represents an allocated kernel object.

    object_type is the type of kernel object.
    phys_address is the physical memory address of the kernel object.

    Kernel objects can have multiple caps (and caps can have multiple addresses).
    The cap referred to here is the original cap that is allocated when the
    kernel object is first allocate.
    The cap_slot refers to the specific slot in which this cap resides.
    The cap_address refers to a cap address that addresses this cap.
    The cap_address is is intended to be valid within the context of the
    initial task.
    """
    object_type: int
    cap_slot: int
    cap_addr: int
    phys_addr: int
    name: str


def assert_objects_adjacent(lst: List[KernelObject]) -> None:
    """check that all objects in the list are adjacent"""
    prev_cap_addr = lst[0].cap_addr
    for o in lst[1:]:
        assert o.cap_addr == prev_cap_addr + 1
        prev_cap_addr = o.cap_addr


def human_size_strict(size: int) -> str:
    """Product a 'human readable' string for the size.

    'strict' means that it must be simply represented.
    Specifically, it must be a multiple of standard power-of-two.
    (e.g. KiB, MiB, GiB, TiB, PiB, EiB)
    """
    if size > (1 << 70):
        raise ValueError("size is too large for human representation")
    for bits, label in (
        (60, "EiB"),
        (50, "PiB"),
        (40, "TiB"),
        (30, "GiB"),
        (20, "MiB"),
        (10, "KiB"),
        (0, "bytes"),
    ):
        base = 1 << bits
        if size > base:
            if base > 0:
                count, extra = divmod(size, base)
                if extra != 0:
                    raise ValueError(f"size 0x{size:x} is not a multiple of standard power-of-two")
            else:
                count = size
            return f"{count:,d} {label}"
    raise Exception("should never reach here")


class InitSystem:
    def __init__(
            self,
            kernel_config: KernelConfig,
            cnode_cap: int,
            cnode_mask: int,
            first_available_cap_slot: int,
            kernel_object_allocator: KernelObjectAllocator,
            kernel_boot_info: KernelBootInfo,
            invocations: List[Sel4Invocation],
            cap_address_names: Dict[int, str],
        ):
        self._cnode_cap = cnode_cap
        self._cnode_mask = cnode_mask
        self._kernel_config = kernel_config
        self._koa = kernel_object_allocator
        self._invocations = invocations
        self._cap_slot = first_available_cap_slot
        self._last_fixed_address = 0
        self._device_untyped = sorted([FixedUntypedAlloc(ut) for ut in kernel_boot_info.untyped_objects if ut.is_device])
        self._cap_address_names = cap_address_names
        self._objects: List[KernelObject] = []

    def reserve(self, allocations: List[Tuple[UntypedObject, int]]) -> None:
        for alloc_ut, alloc_phys_addr in allocations:
            for ut in self._device_untyped:
                if alloc_ut == ut._ut:
                    break
            else:
                raise Exception(f"Allocation {alloc_ut} ({alloc_phys_addr:x}) not in any device untyped")

            if not (ut._ut.region.base <= alloc_phys_addr <= ut._ut.region.end):
                raise Exception(f"Allocation {alloc_ut} ({alloc_phys_addr:x}) not in untyped region {ut._ut.region}")

            ut.watermark = alloc_phys_addr


    def allocate_fixed_objects(self, phys_address: int, object_type: int, count: int, names: List[str]) -> List[KernelObject]:
        """

        Note: Fixed objects must be allocated in order!
        """
        assert phys_address >= self._last_fixed_address
        assert object_type in FIXED_OBJECT_SIZES
        assert count == len(names)
        alloc_size = FIXED_OBJECT_SIZES[object_type]

        for ut in self._device_untyped:
            if phys_address in ut:
                break
        else:
            raise Exception(f"{phys_address=:x} not in any device untyped")

        if phys_address < ut.watermark:
            raise Exception(f"{phys_address=:x} is below watermark")

        if ut.watermark != phys_address:
            # If the watermark isn't at the right spot, then we need to
            # create padding objects until it is.
            padding_required = phys_address - ut.watermark
            # We are restricted in how much we can pad:
            # 1: Untyped objects must be power-of-two sized.
            # 2: Untyped objects must be aligned to their size.
            padding_sizes = []
            # We have two potential approaches for how we pad.
            # 1: Use largest objects possible respecting alignment
            # and size restrictions.
            # 2: Use a fixed size object multiple times. This will
            # create more objects, but as same sized objects can be
            # create in a batch, required fewer invocations.
            # For now we choose #1
            wm = ut.watermark
            while padding_required > 0:
                wm_lsb = lsb(wm)
                sz_msb = msb(padding_required)
                pad_object_size = 1 << min(wm_lsb, sz_msb)
                padding_sizes.append(pad_object_size)
                wm += pad_object_size
                padding_required -= pad_object_size

            for sz in padding_sizes:
                self._invocations.append(Sel4UntypedRetype(
                        ut._ut.cap,
                        SEL4_UNTYPED_OBJECT,
                        int(log2(sz)),
                        self._cnode_cap,
                        1,
                        1,
                        self._cap_slot,
                        1
                ))
                self._cap_slot += 1

        object_cap = self._cap_slot
        self._cap_slot += 1
        self._invocations.append(Sel4UntypedRetype(
                ut._ut.cap,
                object_type,
                0,
                self._cnode_cap,
                1,
                1,
                object_cap,
                1
        ))

        ut.watermark = phys_address + alloc_size
        self._last_fixed_address = phys_address + alloc_size
        cap_address = self._cnode_mask | object_cap
        self._cap_address_names[cap_address] = names[0]
        kernel_objects = [KernelObject(object_type, object_cap, cap_address, phys_address, names[0])]
        self._objects += kernel_objects
        return kernel_objects

    def allocate_objects(self, object_type: int, names: List[str], size: Optional[int] = None) -> List[KernelObject]:
        count = len(names)
        if object_type in FIXED_OBJECT_SIZES:
            assert size is None
            alloc_size = FIXED_OBJECT_SIZES[object_type]
            api_size = 0
        elif object_type in (SEL4_CNODE_OBJECT, SEL4_SCHEDCONTEXT_OBJECT):
            assert size is not None
            assert is_power_of_two(size)
            api_size = int(log2(size))
            alloc_size = size * SLOT_SIZE
        else:
            raise Exception(f"Invalid object type: {object_type}")
        allocation = self._koa.alloc(alloc_size, count)
        base_cap_slot = self._cap_slot
        self._cap_slot += count
        to_alloc = count
        alloc_cap_slot = base_cap_slot
        while to_alloc:
            call_count = min(to_alloc, self._kernel_config.fan_out_limit)
            self._invocations.append(Sel4UntypedRetype(
                    allocation.untyped_cap_address,
                    object_type,
                    api_size,
                    self._cnode_cap,
                    1,
                    1,
                    alloc_cap_slot,
                    call_count
            ))
            to_alloc -= call_count
            alloc_cap_slot += call_count
        kernel_objects = []
        phys_addr = allocation.phys_addr
        for idx in range(count):
            cap_slot = base_cap_slot + idx
            cap_address = self._cnode_mask | cap_slot
            name = names[idx]
            self._cap_address_names[cap_address] = name
            kernel_objects.append(KernelObject(object_type, cap_slot, cap_address, phys_addr, name))
            phys_addr += alloc_size

        self._objects += kernel_objects
        return kernel_objects


@dataclass(frozen=True)
class Region:
    name: str
    addr: int
    data: bytearray

    def __repr__(self) -> str:
        return f"<Region name={self.name} addr=0x{self.addr:x} size={len(self.data)}>"


@dataclass
class BuiltSystem:
    number_of_system_caps: int
    invocation_data_size: int
    bootstrap_invocations: List[Sel4Invocation]
    system_invocations: List[Sel4Invocation]
    kernel_boot_info: KernelBootInfo
    reserved_region: MemoryRegion
    cap_lookup: Dict[int, str]
    tcb_caps: List[int]
    sched_caps: List[int]
    ntfn_caps: List[int]
    regions: List[Region]
    kernel_objects: List[KernelObject]
    initial_task_virt_region: MemoryRegion
    initial_task_phys_region: MemoryRegion


def _get_full_path(filename: Path, search_paths: List[Path]) -> Path:
    for search_path in search_paths:
        full_path = search_path / filename
        if full_path.exists():
            return full_path
    else:
        raise UserError(f"Error: unable to find program image: '{filename}'")


def build_system(
        kernel_config: KernelConfig,
        kernel_elf: ElfFile,
        sysinit_elf: ElfFile,
        system: SystemDescription,
        invocation_table_size: int,
        system_cnode_size: int,
        search_paths: List[Path],
    ) -> BuiltSystem:
    """Build system as description by the inputs, with a 'BuiltSystem' object as the output."""
    assert is_power_of_two(system_cnode_size)
    assert invocation_table_size % kernel_config.minimum_page_size == 0
    assert invocation_table_size <= MAX_SYSTEM_INVOCATION_SIZE

    invocation: Sel4Invocation

    cap_address_names = {}
    cap_address_names[INIT_NULL_CAP_ADDRESS] = "null"
    cap_address_names[INIT_TCB_CAP_ADDRESS] = "TCB: init"
    cap_address_names[INIT_CNODE_CAP_ADDRESS] = "CNode: init"
    cap_address_names[INIT_VSPACE_CAP_ADDRESS] = "VSpace: init"
    cap_address_names[INIT_ASID_POOL_CAP_ADDRESS] = "ASID Pool: init"
    cap_address_names[IRQ_CONTROL_CAP_ADDRESS] = "IRQ Control"
    cap_address_names[DOMAIN_CAP_ADDRESS] = "Domain"

    system_cnode_bits = int(log2(system_cnode_size))

    # Emulate kernel boot

    ## Determine physical memory region used by the sysinit task
    initial_task_size = phys_mem_region_from_elf(sysinit_elf, kernel_config.minimum_page_size).size

    ## Get the elf files for each pd:
    pd_elf_files = {
        pd: ElfFile.from_path(_get_full_path(pd.program_image, search_paths))
        for pd in system.protection_domains
    }
    ### Here we should validate that ELF files

    ## Determine physical memory region for 'reserved' memory.
    #
    # The 'reserved' memory region will not be touched by seL4 during boot
    # and allows the sysinit (root task) to create memory regions
    # from this area, which can then be made available to the appropriate
    # protection domains
    pd_elf_size = sum([
        sum([r.size for r in phys_mem_regions_from_elf(elf, kernel_config.minimum_page_size)])
        for elf in pd_elf_files.values()
    ])
    reserved_size = invocation_table_size + pd_elf_size

    # Now that the size is determine, find a free region in the physical memory
    # space.
    available_memory, kernel_boot_region = emulate_kernel_boot_partial(
        kernel_config,
        kernel_elf,
    )

    # The kernel relies on the reserved region being allocated above the kernel
    # boot/ELF region, so we have the end of the kernel boot region as the lower
    # bound for allocating the reserved region.
    reserved_base = available_memory.allocate_from(reserved_size, kernel_boot_region.end)
    assert kernel_boot_region.base < reserved_base
    # The kernel relies on the initial task being allocated above the reserved
    # region, so we have the address of the end of the reserved region as the
    # lower bound for allocating the initial task.
    initial_task_phys_base = available_memory.allocate_from(initial_task_size, reserved_base + reserved_size)
    assert reserved_base < initial_task_phys_base

    initial_task_phys_region = MemoryRegion(initial_task_phys_base, initial_task_phys_base + initial_task_size)
    initial_task_virt_region = virt_mem_region_from_elf(sysinit_elf, kernel_config.minimum_page_size)

    reserved_region = MemoryRegion(reserved_base, reserved_base + reserved_size)

    # Now that the reserved region has been allocated we can determine the specific
    # region of physical memory required for the inovcation table itself, and
    # all the ELF segments
    invocation_table_region = MemoryRegion(reserved_base, reserved_base + invocation_table_size)

    phys_addr_next = invocation_table_region.end
    # Now we create additional MRs (and mappings) for the ELF files.
    pd_elf_regions = {}
    for pd in system.protection_domains:
        elf_regions: List[Tuple[int, bytearray, str]] = []
        seg_idx = 0
        for segment in pd_elf_files[pd].segments:
            if not segment.is_loadable:
                continue

            perms = ""
            if segment.is_readable:
                perms += "r"
            if segment.is_writable:
                perms += "w"
            if segment.is_executable:
                perms += "x"

            elf_regions.append((phys_addr_next, segment.data, perms))
            phys_addr_next = round_up(phys_addr_next + len(segment.data), kernel_config.minimum_page_size)

            # base_vaddr = round_down(segment.virt_addr, kernel_config.minimum_page_size)
            # end_vaddr = round_up(segment.virt_addr + segment.mem_size, kernel_config.minimum_page_size)
            # aligned_size = end_vaddr - base_vaddr
            # name = f"ELF:{pd.name}-{seg_idx}"
            # mr = SysMemoryRegion(name, "small", aligned_size // kernel_config.minimum_page_size, phys_addr_next)
            # seg_idx += 1
            # phys_addr_next += aligned_size
            # system.mr_by_name[mr.name] = mr
            # system.memory_regions.append(mr)

            # mp = SysMap(mr.name, base_vaddr, perms=perms, cached=True)
            # pd.maps.append(mp)
        pd_elf_regions[pd] = tuple(elf_regions)


    # 1.3 With both the initial task region and reserved region determined the kernel
    # boot can be emulated. This provides the boot info information which is needed
    # for the next steps
    kernel_boot_info = emulate_kernel_boot(
        kernel_config,
        kernel_elf,
        initial_task_phys_region,
        initial_task_virt_region,
        reserved_region
    )

    for ut in kernel_boot_info.untyped_objects:
        dev_str = " (device)" if ut.is_device else ""
        cap_address_names[ut.cap] = f"Untyped @ 0x{ut.region.base:x}:0x{ut.region.size:x}{dev_str}"

    # X. The kernel boot info allows us to create an allocator for kernel objects
    koa = KernelObjectAllocator(kernel_boot_info)

    # 2. Now that the available resources are known it is possible to proceed with the
    # sysinit task boot strap.
    #
    # The bootstrap of the sysinit task works in two phases:
    #
    #   1. Setting up the sysinit task's CSpace
    #   2. Making the system invocation table available in the sysinit task's address
    #   space.

    # 2.1 The sysinit task's CSpace consists of two CNodes: a/ the initial task CNode
    # which consists of all the fixed initial caps along with caps for the
    # object create during kernel bootstrap, and b/ the system CNode, which
    # contains caps to all objects that will be created in this process.
    # The system CNode is of `system_cnode_size`. (Note: see also description
    # on how `system_cnode_size` is iteratively determined).
    #
    # The system CNode is not available at startup and must be created (by retyping
    # memory from an untyped object). Once created the two CNodes must be arranged
    # as a tree such that the slots in both CNodes are addressable.
    #
    # The system CNode shall become the root of the CSpace. The initial CNode shall
    # be copied to slot zero of the system CNode. In this manner all caps in the initial
    # CNode will keep their original cap addresses. This isn't required but it makes
    # allocation, debugging and reasoning about the system more straight forward.
    #
    # 2.1.1: Allocate the *root* CNode. It is two entries:
    #  slot 0: the existing init cnode
    #  slot 1: our main system cnode
    root_cnode_bits = 1
    root_cnode_allocation = koa.alloc((1 << root_cnode_bits) * (1 << SLOT_BITS))
    root_cnode_cap =  kernel_boot_info.first_available_cap
    cap_address_names[root_cnode_cap] = "CNode: root"

    # 2.1.2: Allocate the *system* CNode. It is the CNode that
    # will have enough slots for all required caps.
    system_cnode_allocation = koa.alloc(system_cnode_size * (1 << SLOT_BITS))
    system_cnode_cap = kernel_boot_info.first_available_cap + 1
    cap_address_names[system_cnode_cap] = "CNode: system"

    # 2.1.3: Now that we've allocated the space for these we generate
    # the actual systems calls.
    #
    # First up create the root cnode
    bootstrap_invocations: List[Sel4Invocation] = []

    bootstrap_invocations.append(Sel4UntypedRetype(
            root_cnode_allocation.untyped_cap_address,
            SEL4_CNODE_OBJECT,
            root_cnode_bits,
            INIT_CNODE_CAP_ADDRESS,
            0,
            0,
            root_cnode_cap,
            1
    ))

    # 2.1.4: Now insert a cap to the initial CNode into slot zero of the newly
    # allocated root CNode. It uses sufficient guard bits to ensure it is
    # completed padded to word size.
    #
    # guard width is the lower 6 bits of the data word, upper bits are the guard value itself
    # which is now outdated and no longer used
    #
    # Since the initial CNode will become a L2 CNode (the root CNode being L1), the guard for it
    # will be: 64 - (bits translated by root CNode) - (bits translated by the initial CNode)
    guard = kernel_config.cap_address_bits - root_cnode_bits - kernel_config.init_cnode_bits
    bootstrap_invocations.append(Sel4CnodeMint(
        root_cnode_cap,
        0,
        root_cnode_bits,
        INIT_CNODE_CAP_ADDRESS,
        INIT_CNODE_CAP_ADDRESS,
        kernel_config.cap_address_bits,
        SEL4_RIGHTS_ALL,
        guard
    ))

    # 2.1.5: Now it is possible to switch our root CNode (of the init task)
    # to the newly created root CNode. We have a zero width guard. This
    # CNode represents the top bit of any cap addresses.
    root_guard = 0
    bootstrap_invocations.append(Sel4TcbSetSpace(
        INIT_TCB_CAP_ADDRESS,
        INIT_NULL_CAP_ADDRESS,
        root_cnode_cap,
        root_guard,
        INIT_VSPACE_CAP_ADDRESS,
        0
    ))

    # 2.1.6: Now we can create our new system CNode. We will place it into
    # a temporary cap slot in the initial CNode to start with.
    bootstrap_invocations.append(Sel4UntypedRetype(
        system_cnode_allocation.untyped_cap_address,
        SEL4_CNODE_OBJECT,
        system_cnode_bits,
        INIT_CNODE_CAP_ADDRESS,
        0,
        0,
        system_cnode_cap,
        1
    ))

    # 2.1.7: Now that we have created the object, we can 'mutate' it
    # to the correct place:
    # Slot #1 of the new root cnode
    # FIXME: this is a CNode Mint operation, not mutate
    guard = kernel_config.cap_address_bits - root_cnode_bits - system_cnode_bits
    system_cap_address_mask = 1 << (kernel_config.cap_address_bits - 1)
    bootstrap_invocations.append(Sel4CnodeMint(
        root_cnode_cap,
        1,
        root_cnode_bits,
        INIT_CNODE_CAP_ADDRESS,
        system_cnode_cap,
        kernel_config.cap_address_bits,
        SEL4_RIGHTS_ALL,
        guard
    ))

    # 2.2 At this point it is necessary to get the frames containing the
    # main system invocations into the virtual address space. (Remember the
    # invocations we are writing out here actually _execute_ at run time!
    # It is a bit weird that we talk about mapping in the invocation data
    # before we have even generated the invocation data!).
    #
    # This needs a few steps:
    #
    # 1. Turn untyped into page objects
    # 2. Map the page objects into the address space
    #

    # 2.2.1: The memory for the system invocation data resides at the start
    # of the reserved region. We can retype multiple frames as a time (
    # which reduces the number of invocations we need). However, it is possible
    # that the region spans multiple untyped objects.
    # At this point in time we assume we will map the area using the minimum
    # page size. It would be good in the future to use super pages (when
    # it makes sense to - this would reduce memory usage, and the number of
    # invocations required to set up the address space
    pages_required= invocation_table_size // kernel_config.minimum_page_size
    remaining_pages = pages_required
    invocation_table_allocations = []
    phys_addr = invocation_table_region.base
    base_page_cap = 0
    for pta in range(base_page_cap, base_page_cap + pages_required):
        cap_address_names[system_cap_address_mask | pta] = "SmallPage: sysinit invocation table"

    cap_slot = base_page_cap
    for ut in (ut for ut in kernel_boot_info.untyped_objects if ut.is_device):
        ut_pages = ut.region.size // kernel_config.minimum_page_size
        retype_page_count = min(ut_pages, remaining_pages)
        assert retype_page_count <= kernel_config.fan_out_limit
        bootstrap_invocations.append(Sel4UntypedRetype(
                ut.cap,
                SEL4_SMALL_PAGE_OBJECT,
                0,
                root_cnode_cap,
                1,
                1,
                cap_slot,
                retype_page_count
        ))

        remaining_pages -= retype_page_count
        cap_slot += retype_page_count
        phys_addr += retype_page_count * kernel_config.minimum_page_size
        invocation_table_allocations.append((ut, phys_addr))
        if remaining_pages == 0:
            break

    # 2.2.1: Now that physical pages have been allocated it is possible to setup
    # the virtual memory objects so that the pages can be mapped into virtual memory
    # At this point we map into the arbitrary address of 0x0.8000.0000 (i.e.: 2GiB)
    # We arbitrary limit the maximum size to be 128MiB. This allows for at least 1 million
    # invocations to occur at system startup. This should be enough for any reasonable
    # sized system.
    #
    # Before mapping it is necessary to install page tables that can cover the region.
    page_tables_required = round_up(invocation_table_size, SEL4_LARGE_PAGE_SIZE) // SEL4_LARGE_PAGE_SIZE
    page_table_allocation = koa.alloc(SEL4_PAGE_TABLE_SIZE, page_tables_required)
    base_page_table_cap = cap_slot

    for pta in range(base_page_table_cap, base_page_table_cap + page_tables_required):
        cap_address_names[system_cap_address_mask | pta] = "PageTable: sysinit"

    assert page_tables_required <= kernel_config.fan_out_limit
    bootstrap_invocations.append(Sel4UntypedRetype(
            page_table_allocation.untyped_cap_address,
            SEL4_PAGE_TABLE_OBJECT,
            0,
            root_cnode_cap,
            1,
            1,
            cap_slot,
            page_tables_required
    ))
    cap_slot += page_tables_required

    # Now that the page tables are allocated they can be mapped into vspace
    vaddr = 0x8000_0000
    invocation = Sel4PageTableMap(system_cap_address_mask | base_page_table_cap, INIT_VSPACE_CAP_ADDRESS, vaddr, SEL4_ARM_DEFAULT_VMATTRIBUTES)
    invocation.repeat(page_tables_required, page_table=1, vaddr=SEL4_LARGE_PAGE_SIZE)
    bootstrap_invocations.append(invocation)

    # Finally, once the page tables are allocated the pages can be mapped
    vaddr = 0x8000_0000
    invocation = Sel4PageMap(system_cap_address_mask | base_page_cap, INIT_VSPACE_CAP_ADDRESS, vaddr, SEL4_RIGHTS_READ, SEL4_ARM_DEFAULT_VMATTRIBUTES | SEL4_ARM_EXECUTE_NEVER)
    invocation.repeat(pages_required, page=1, vaddr=kernel_config.minimum_page_size)
    bootstrap_invocations.append(invocation)


    # 3. Now we can start setting up the system based on the information
    # the user provided in the system xml.
    #
    # Create all the objects:
    #
    #  TCBs: one per PD
    #  Endpoints: one per PD with a PP + one for the monitor
    #  IPC Buffers: one per PD with an EP + one per thread
    #  TLS: one per PD sized to accomodate threads
    #  Notification: one per PD
    #  VSpaces: one per PD
    #  CNodes: one per PD
    #  Small Pages:
    #     one per pd for IPC buffer
    #     as needed by MRs
    #  Large Pages:
    #     as needed by MRs
    #  Page table structs:
    #     as needed by protection domains based on mappings required


    phys_addr_next = reserved_base + invocation_table_size
    # Now we create additional MRs (and mappings) for the ELF files.
    regions: List[Region] = []
    extra_mrs = []
    pd_extra_maps: Dict[ProtectionDomain, Tuple[SysMap, ...]] = {pd: tuple() for pd in system.protection_domains}
    for pd in system.protection_domains:
        seg_idx = 0
        for segment in pd_elf_files[pd].segments:
            if not segment.is_loadable:
                continue

            regions.append(Region(f"PD-ELF {pd.name}-{seg_idx}", phys_addr_next, segment.data))

            perms = ""
            if segment.is_readable:
                perms += "r"
            if segment.is_writable:
                perms += "w"
            if segment.is_executable:
                perms += "x"

            base_vaddr = round_down(segment.virt_addr, kernel_config.minimum_page_size)
            end_vaddr = round_up(segment.virt_addr + segment.mem_size, kernel_config.minimum_page_size)
            aligned_size = end_vaddr - base_vaddr
            name = f"ELF:{pd.name}-{seg_idx}"
            mr = SysMemoryRegion(name, aligned_size, 0x1000, aligned_size // 0x1000, phys_addr_next)
            seg_idx += 1
            phys_addr_next += aligned_size
            extra_mrs.append(mr)

            mp = SysMap(mr.name, base_vaddr, perms=perms, cached=True, element=None)
            pd_extra_maps[pd] += (mp, )

    # Create MRs for TLS, stack and IPC buffer areas
    stack_by_pd: Dict[ProtectionDomain, int] = {}
    tls_by_pd: Dict[ProtectionDomain, int] = {}
    for pd in system.protection_domains:
        tls_segments = pd_elf_files[pd].find_segment_all(SegmentType.PT_TLS)
        if len(tls_segments) != 1:
            raise Exception(f"ELF for {pd} contains more than 1 TLS segment!")
        tls_seg = tls_segments[0]

        # sel4cp_thread_entry() hardcodes the alignment to 8 bytes, check that this
        # assumption holds true. The real solution is to pass a pointer to the TLS
        # PHdr and not have to worry about this, but oh well.
        assert(tls_seg.alignment == (kernel_config.word_size / 8))
        
        # We make the start of the TLS memory area page-aligned, so check
        # that this plays well with the TLS alignment
        assert(kernel_config.minimum_page_size % tls_seg.alignment == 0)

        # Is PD a child PD, and if so does the root PD have threads support?
        is_child_pd_threaded = pd.parent and pd.parent.threads > 0
        threads = pd.parent.threads if is_child_pd_threaded else 0

        # OK - now allocate the TLS regions in the VSpace.
        base_vaddr, aligned_size = ProtectionDomainVSpace.alloc_vspace_tls(threads, tls_seg.mem_size, tls_seg.alignment)
        tls_mr = SysMemoryRegion(
            f"{pd.name}: TLS",
            aligned_size,
            kernel_config.minimum_page_size,
            aligned_size // kernel_config.minimum_page_size,
            None 
        )
        tls_mp = SysMap(tls_mr.name, base_vaddr, perms="rw", cached=True, element=None)
        tls_by_pd[pd] = base_vaddr

        # Allocate the IPC Buffers for this PD.
        base_vaddr, aligned_size = ProtectionDomainVSpace.alloc_vspace_ipc_buffer(threads)
        ipc_buffer_mr = SysMemoryRegion(
            f"{pd.name}: IPC Buffers",
            aligned_size,
            kernel_config.minimum_page_size,
            aligned_size // kernel_config.minimum_page_size,
            None
        )
        ipc_buffer_mp = SysMap(ipc_buffer_mr.name, base_vaddr, perms="rw", cached=True, element=None)

        extra_mrs.extend([tls_mr, ipc_buffer_mr])
        pd_extra_maps[pd] += (tls_mp, ipc_buffer_mp, )

        # Allocate the stack areas, note that since we have guard pages in between
        # each stack, we cannot allocate it all at once in a single contiguous region.
        nstacks = pd.parent.threads if is_child_pd_threaded else 1 # spawnable threads might join us in the VSpace, so if that is a possibility allocate space for them, otherwise its just use, the lonely orphan child pd with no loving parents :(
        for i in range(nstacks):
            base_vaddr, aligned_size = ProtectionDomainVSpace.alloc_thread_stack(i)
            name = f"{pd.name}: Stack {i}"
            mr = SysMemoryRegion(
                name,
                aligned_size,
                kernel_config.minimum_page_size,
                aligned_size // kernel_config.minimum_page_size,
                None
            )
            mp = SysMap(name, base_vaddr, perms="rw", cached=True, element=None)
            extra_mrs.append(mr)
            pd_extra_maps[pd] += (mp, )

            if i == 0:
                stack_by_pd[pd] = base_vaddr

    all_mrs = system.memory_regions + tuple(extra_mrs)
    all_mr_by_name = {mr.name: mr for mr in all_mrs}

    system_invocations: List[Sel4Invocation] = []
    init_system = InitSystem(kernel_config, root_cnode_cap, system_cap_address_mask, cap_slot, koa, kernel_boot_info, system_invocations, cap_address_names)
    init_system.reserve(invocation_table_allocations)

    SUPPORTED_PAGE_SIZES = (0x1_000, 0x200_000)
    SUPPORTED_PAGE_OBJECTS = (SEL4_SMALL_PAGE_OBJECT, SEL4_LARGE_PAGE_OBJECT)
    PAGE_OBJECT_BY_SIZE = dict(zip(SUPPORTED_PAGE_SIZES, SUPPORTED_PAGE_OBJECTS))
    # 3.1 Work out how many regular (non-fixed) page objects are required
    page_names_by_size: Dict[int, List[str]] = {
        page_size: [] for page_size in SUPPORTED_PAGE_SIZES
    }
    for mr in all_mrs:
        if mr.phys_addr is not None:
            continue
        page_size_human = human_size_strict(mr.page_size)
        page_names_by_size[mr.page_size] +=  [f"Page({page_size_human}): MR={mr.name} #{idx}" for idx in range(mr.page_count)]

    page_objects: Dict[int, List[KernelObject]] = {}

    for page_size, page_object in reversed(list(zip(SUPPORTED_PAGE_SIZES, SUPPORTED_PAGE_OBJECTS))):
        page_objects[page_size] = init_system.allocate_objects(page_object, page_names_by_size[page_size])


    pg_idx: Dict[int, int] = {sz: 0 for sz in SUPPORTED_PAGE_SIZES}
    mr_pages: Dict[SysMemoryRegion, List[KernelObject]] = {mr: [] for mr in all_mrs}
    for mr in all_mrs:
        if mr.phys_addr is not None:
            continue
        idx = pg_idx[mr.page_size]
        mr_pages[mr] = [page_objects[mr.page_size][i] for i in range(idx, idx + mr.page_count)]
        pg_idx[mr.page_size] += mr.page_count

    # 3.2 Now allocate all the fixed MRs

    # First we need to find all the requested pages and sorted them
    fixed_pages = []
    for mr in all_mrs:
        if mr.phys_addr is None:
            continue
        phys_addr = mr.phys_addr
        for idx in range(mr.page_count):
            fixed_pages.append((phys_addr, mr))
            phys_addr += mr_page_bytes(mr)

    fixed_pages.sort()

    # FIXME: At this point we can recombine them into
    # groups to optimize allocation

    for phys_addr, mr in fixed_pages:
        if mr.page_size not in SUPPORTED_PAGE_SIZES:
            raise Exception(f"Invalid page_size: 0x{mr.page_size:x} for mr {mr}")
        obj_type = PAGE_OBJECT_BY_SIZE[mr.page_size]
        obj_type_name = f"Page({human_size_strict(mr.page_size)})"
        name = f"{obj_type_name}: MR={mr.name} @ {phys_addr:x}"
        page = init_system.allocate_fixed_objects(phys_addr, obj_type, 1, names=[name])[0]
        mr_pages[mr].append(page)
    
    # Order of system CNode:
    # TCB -> SC -> Reply Objs -> Endpoints (for PDs that have an EP) -> Ntfn
    # These caps are laid out sequentially in the CNode in order to make use of
    # "repeating" invocations, which saves the final image memory usage.
    tcb_names = [f"TCB: PD={pd.name}" for pd in system.protection_domains]
    tcb_objects = init_system.allocate_objects(SEL4_TCB_OBJECT, tcb_names)
    tcb_caps = [tcb_obj.cap_addr for tcb_obj in tcb_objects]

    schedcontext_names = [f"SchedContext: PD={pd.name}" for pd in system.protection_domains]
    schedcontext_objects = init_system.allocate_objects(SEL4_SCHEDCONTEXT_OBJECT, schedcontext_names, size=PD_SCHEDCONTEXT_SIZE)
    schedcontext_caps = [sc.cap_addr for sc in schedcontext_objects]
    reply_names = [f"Reply: PD={pd.name}" for pd in system.protection_domains]
    reply_objects = init_system.allocate_objects(SEL4_REPLY_OBJECT, reply_names)
    pd_reply_objects = reply_objects
    pds_with_endpoints = [pd for pd in system.protection_domains if pd.needs_ep]
    endpoint_names = [f"EP: PD={pd.name}" for pd in pds_with_endpoints]
    endpoint_objects = init_system.allocate_objects(SEL4_ENDPOINT_OBJECT, endpoint_names)
    pd_endpoint_objects = dict(zip(pds_with_endpoints, endpoint_objects))
    notification_names = [f"Notification: PD={pd.name}" for pd in system.protection_domains]
    notification_objects = init_system.allocate_objects(SEL4_NOTIFICATION_OBJECT, notification_names)
    notification_objects_by_pd = dict(zip(system.protection_domains, notification_objects))
    notification_caps = [ntfn.cap_addr for ntfn in notification_objects]

    # PDs with children
    pd_children = {
        pd: []
        for pd in system.protection_domains
    }
    for pd in system.protection_domains:
        for maybe_child_pd in system.protection_domains:
            if maybe_child_pd.parent is pd:
                pd_children[pd].append(maybe_child_pd)


    # Determine number of upper directory / directory / page table objects required
    #
    # Upper directory (level 3 table) is based on how many 512 GiB parts of the address
    # space is covered (normally just 1!).
    #
    # Page directory (level 2 table) is based on how many 1,024 MiB parts of
    # the address space is covered
    #
    # Page table (level 3 table) is based on how many 2 MiB parts of the
    # address space is covered (excluding any 2MiB regions covered by large
    # pages).

    uds = []
    ds = []
    pts = []
    for pd_idx, pd in enumerate(system.protection_domains):
        upper_directory_vaddrs = set()
        directory_vaddrs = set()
        page_table_vaddrs = set()

        # For each page, in each map determine we determine
        # which upper directory, directory and page table is resides
        # in, and then page sure this is set
        vaddrs = []
        for map in (pd.maps + pd_extra_maps[pd]):
            mr = all_mr_by_name[map.mr]
            vaddr = map.vaddr
            for _ in range(mr.page_count):
                vaddrs.append((vaddr, mr.page_size))
                vaddr += mr_page_bytes(mr)

        for vaddr, page_size in vaddrs:
            upper_directory_vaddrs.add(mask_bits(vaddr, 12 + 9 + 9 + 9))
            directory_vaddrs.add(mask_bits(vaddr, 12 + 9 + 9))
            if page_size == 0x1_000:
                page_table_vaddrs.add(mask_bits(vaddr, 12 + 9))
        uds += [(pd_idx, vaddr) for vaddr in sorted(upper_directory_vaddrs)]
        ds += [(pd_idx, vaddr) for vaddr in sorted(directory_vaddrs)]
        pts += [(pd_idx, vaddr) for vaddr in sorted(page_table_vaddrs)]

    pd_names = [pd.name for pd in system.protection_domains]
    vspace_names = [f"VSpace: PD={pd.name}" for pd in system.protection_domains]

    vspace_objects = init_system.allocate_objects(SEL4_VSPACE_OBJECT, vspace_names)
    vspace_objects_by_pd = dict(zip(system.protection_domains, vspace_objects))

    ud_names = [f"PageUpperDirectory: PD={pd_names[pd_idx]} VADDR=0x{vaddr:x}" for pd_idx, vaddr in uds]
    ud_objects = init_system.allocate_objects(SEL4_PAGE_UPPER_DIRECTORY_OBJECT, ud_names)

    d_names = [f"PageDirectory: PD={pd_names[pd_idx]} VADDR=0x{vaddr:x}" for pd_idx, vaddr in ds]
    d_objects = init_system.allocate_objects(SEL4_PAGE_DIRECTORY_OBJECT, d_names)

    pt_names = [f"PageTable: PD={pd_names[pd_idx]} VADDR=0x{vaddr:x}" for pd_idx, vaddr in pts]
    pt_objects = init_system.allocate_objects(SEL4_PAGE_TABLE_OBJECT, pt_names)

    # Create PD CNodes - all CNode objects for PDs are the same size: 128 slots.
    cnode_names = [f"CNode: PD={pd.name}" for pd in system.protection_domains]
    cnode_objects = init_system.allocate_objects(SEL4_CNODE_OBJECT, cnode_names, size=PD_CAP_SIZE)
    cnode_objects_by_pd = dict(zip(system.protection_domains, cnode_objects))


    # PDs that have control of empty threads
    pds_with_threads = [pd for pd in system.protection_domains if pd.threads > 0]
    threads_tcbs = {}
    threads_sc = {}
    threads_cnodes = {}
    threads_replies = {}
    child_vspaces = {}
    for pd in pds_with_threads:
        # Need to let the thread know at runtime how many threads it's controlling
        pd_elf_files[pd].write_symbol("libsel4cp_max_threads", pack("<Q", pd.threads))

        # Create spawnable thread TCBs
        threads_tcbs_names = [f"Thread TCB: PD={pd.name} #{i}" for i in range(pd.threads)]
        threads_tcbs[pd] = init_system.allocate_objects(SEL4_TCB_OBJECT, threads_tcbs_names)

        # Create spawnable thread Scheduling Contexts
        threads_sc_names = [f"Thread SC: PD={pd.name} #{i}" for i in range(pd.threads)]
        threads_sc[pd] = init_system.allocate_objects(SEL4_SCHEDCONTEXT_OBJECT, threads_sc_names, size=PD_SCHEDCONTEXT_SIZE)

        # Create spawnable thread CNodes - they are all 4 slots for now.
        threads_cnode_names = [f"Thread CNode: PD={pd.name} #{i}" for i in range(pd.threads)]
        threads_cnodes[pd] = init_system.allocate_objects(SEL4_CNODE_OBJECT, threads_cnode_names, size=EMPTY_THREAD_CAP_SIZE)

        # Create Reply objects for maximum amount of threads we support
        threads_replies_names = [f"Thread Reply: PD={pd.name} #{i}" for i in range(pd.threads)]
        threads_replies[pd] = init_system.allocate_objects(SEL4_REPLY_OBJECT, threads_replies_names)

        # Collect VSpaces of child PDs
        child_vspaces[pd] = []
        for child_pd,vspace in zip(system.protection_domains, vspace_objects):
            if child_pd.parent == pd:
                child_vspaces[pd].append((child_pd.pd_id, vspace))


    cap_slot = init_system._cap_slot

    # Create all the necessary interrupt handler objects. These aren't
    # created through retype though!
    irq_cap_addresses: Dict[ProtectionDomain, List[int]] = {pd: [] for pd in system.protection_domains}
    for pd in system.protection_domains:
        for sysirq in pd.irqs:
            cap_address = system_cap_address_mask | cap_slot
            system_invocations.append(
                Sel4IrqControlGet(
                    IRQ_CONTROL_CAP_ADDRESS,
                    sysirq.irq,
                    root_cnode_cap,
                    cap_address,
                    kernel_config.cap_address_bits
                )
            )

            cap_slot += 1
            cap_address_names[cap_address] = f"IRQ Handler: irq={sysirq.irq:d}"
            irq_cap_addresses[pd].append(cap_address)

    # This has to be done prior to minting!
    # for vspace_obj in vspace_objects:
    #     system_invocations.append(Sel4AsidPoolAssign(INIT_ASID_POOL_CAP_ADDRESS, vspace_obj.cap_addr))
    invocation = Sel4AsidPoolAssign(INIT_ASID_POOL_CAP_ADDRESS, vspace_objects[0].cap_addr)
    assert len(vspace_objects) == len(system.protection_domains)
    invocation.repeat(len(vspace_objects), vspace=1)
    system_invocations.append(invocation)

    # Create copies of all caps required via minting.

    # Mint copies of required pages, while also determining what's required
    # for later mapping
    page_descriptors = []
    ipc_buffers_by_pd: Dict[ProtectionDomain, Tuple[int, int]] = {}
    for pd_idx, pd in enumerate(system.protection_domains):
        for mp in (pd.maps + pd_extra_maps[pd]):
            vaddr = mp.vaddr
            mr = all_mr_by_name[mp.mr]
            rights = 0
            attrs = SEL4_ARM_PARITY_ENABLED
            if "r" in mp.perms:
                rights |= SEL4_RIGHTS_READ
            if "w" in mp.perms:
                rights |= SEL4_RIGHTS_WRITE
            if "x" not in mp.perms:
                attrs |= SEL4_ARM_EXECUTE_NEVER
            if mp.cached:
                attrs |= SEL4_ARM_PAGE_CACHEABLE

            assert len(mr_pages[mr]) > 0
            assert_objects_adjacent(mr_pages[mr])

            invocation = Sel4CnodeMint(system_cnode_cap, cap_slot, system_cnode_bits, root_cnode_cap, mr_pages[mr][0].cap_addr, kernel_config.cap_address_bits, rights, 0)
            invocation.repeat(len(mr_pages[mr]), dest_index=1, src_obj=1)
            system_invocations.append(invocation)

            page_descriptors.append((
                system_cap_address_mask | cap_slot,
                pd_idx,
                vaddr,
                rights,
                attrs,
                len(mr_pages[mr]),
                mr_page_bytes(mr)
            ))

            # Remember the cap address of the IPC buffer objects for each PD, we only store the first.
            if mp.mr == f"{pd.name}: IPC Buffers":
                ipc_buffers_by_pd[pd] = (system_cap_address_mask | cap_slot, vaddr)

            for idx in range(len(mr_pages[mr])):
                cap_address_names[system_cap_address_mask | (cap_slot + idx)] = cap_address_names[mr_pages[mr][0].cap_addr + idx] + " (derived)"

            cap_slot += len(mr_pages[mr])

    badged_irq_caps: Dict[ProtectionDomain, List[int]] = {pd: [] for pd in system.protection_domains}
    for notification_obj, pd in zip(notification_objects, system.protection_domains):
        for sysirq in pd.irqs:
            badge = 1 << sysirq.id_
            badged_cap_address = system_cap_address_mask | cap_slot
            system_invocations.append(
                Sel4CnodeMint(
                    system_cnode_cap,
                    cap_slot,
                    system_cnode_bits,
                    root_cnode_cap,
                    notification_obj.cap_addr,
                    kernel_config.cap_address_bits,
                    SEL4_RIGHTS_ALL,
                    badge)
            )
            cap_address_names[badged_cap_address] = cap_address_names[notification_obj.cap_addr] + f" (badge=0x{badge:x})"
            badged_irq_caps[pd].append(badged_cap_address)
            cap_slot += 1

    # Create a fault endpoint cap for each protection domain.
    # For root PDs this shall be the system fault_ep_endpoint_object.
    # For non-root PDs this shall be the parent endpoint.
    badged_fault_ep = system_cap_address_mask | cap_slot
    for idx, pd in enumerate(system.protection_domains, 1):
        is_root = pd.parent is None
        if is_root:
            fault_ep_cap = pd_endpoint_objects[pd].cap_addr
            badge = BADGE_TYPE_FAULT | BADGE_FAULT_ROOT # We use an additional bit to determine if the fault is coming from the root PD
            # itself. This is because PD IDs have a local namespace, which the root PD is NOT apart of.
        else:
            assert pd.pd_id is not None
            assert pd.parent is not None
            fault_ep_cap = pd_endpoint_objects[pd.parent].cap_addr
            badge =  BADGE_TYPE_FAULT | pd.pd_id

        invocation = Sel4CnodeMint(
            system_cnode_cap,
            cap_slot,
            system_cnode_bits,
            root_cnode_cap,
            fault_ep_cap,
            kernel_config.cap_address_bits,
            SEL4_RIGHTS_ALL,
            badge
        )
        system_invocations.append(invocation)
        cap_slot += 1

    final_cap_slot = cap_slot

    ## Minting in the address space
    for pd, notification_obj, cnode_obj in zip(system.protection_domains, notification_objects, cnode_objects):
        obj = pd_endpoint_objects[pd] if pd.needs_ep else notification_obj
        assert INPUT_CAP_IDX < PD_CAP_SIZE
        system_invocations.append(
            Sel4CnodeMint(
                cnode_obj.cap_addr,
                INPUT_CAP_IDX,
                PD_CAP_BITS,
                root_cnode_cap,
                obj.cap_addr,
                kernel_config.cap_address_bits,
                SEL4_RIGHTS_ALL,
                0)
        )

    # TODO: Child PDs shouldnt get this if we don't plan on them handling PPCs...
    assert REPLY_CAP_IDX < PD_CAP_SIZE
    invocation = Sel4CnodeMint(cnode_objects[0].cap_addr, REPLY_CAP_IDX, PD_CAP_BITS, root_cnode_cap, pd_reply_objects[0].cap_addr, kernel_config.cap_address_bits, SEL4_RIGHTS_ALL, 1)
    invocation.repeat(len(system.protection_domains), cnode=1, src_obj=1)
    system_invocations.append(invocation)

    ## Mint access to the vspace cap
    assert VSPACE_CAP_IDX < PD_CAP_SIZE
    invocation = Sel4CnodeMint(cnode_objects[0].cap_addr, VSPACE_CAP_IDX, PD_CAP_BITS, root_cnode_cap, vspace_objects[0].cap_addr, kernel_config.cap_address_bits, SEL4_RIGHTS_ALL, 0)
    invocation.repeat(len(system.protection_domains), cnode=1, src_obj=1)
    system_invocations.append(invocation)

    ## Mint access to interrupt handlers in the PD Cspace
    for cnode_obj, pd in zip(cnode_objects, system.protection_domains):
        for sysirq, irq_cap_address in zip(pd.irqs, irq_cap_addresses[pd]):
            cap_idx = BASE_IRQ_CAP + sysirq.id_
            assert cap_idx < PD_CAP_SIZE
            system_invocations.append(
                Sel4CnodeMint(
                    cnode_obj.cap_addr,
                    cap_idx,
                    PD_CAP_BITS,
                    root_cnode_cap,
                    irq_cap_address,
                    kernel_config.cap_address_bits,
                    SEL4_RIGHTS_ALL,
                    0)
            )

    ## Set up root PD CSpaces
    for cnode_obj, pd in zip(cnode_objects, system.protection_domains):
        # Set up child PD caps first (in the root PD CSpace)
        for maybe_child_pd, maybe_child_tcb, maybe_child_sc, maybe_child_cnode, maybe_child_reply in zip(system.protection_domains, tcb_objects, schedcontext_objects, cnode_objects, reply_objects):
            if maybe_child_pd.parent is pd:
                # Mint the root PD's endpoint into the child PD's CSpace if we support root PPCs
                if pd.accepts_root_ppc:
                    root_pd_ep = pd_endpoint_objects.get(pd)
                    child_pd_badge = BADGE_TYPE_ROOT_PPC | maybe_child_pd.pd_id # remember that first few thread IDs are actually just PD IDs!
                    system_invocations.append(
                        Sel4CnodeMint(
                            maybe_child_cnode.cap_addr,
                            ROOT_PD_EP_CAP_IDX,
                            PD_CAP_BITS, # FIXME @andyb: this is a flaw, CSpace size for PD and threads should be the same if possible
                            root_cnode_cap,
                            root_pd_ep.cap_addr,
                            kernel_config.cap_address_bits,
                            SEL4_RIGHTS_ALL,
                            child_pd_badge)
                    )
                    

                system_invocations.append(
                    Sel4CnodeMint(
                        cnode_obj.cap_addr,
                        BASE_TCB_CAP + maybe_child_pd.pd_id,
                        PD_CAP_BITS,
                        root_cnode_cap,
                        maybe_child_tcb.cap_addr,
                        kernel_config.cap_address_bits,
                        SEL4_RIGHTS_ALL,
                        0)
                )
                
                # We only setup the below caps in the root PD if we plan to support threading
                # "This code looks the exact same as below"
                # Yeah thats true, but this is for PDs, to support threading the root PD needs
                # access to things like SC and CSpace caps, which we provide here for the PDs ONLY.
                if pd in pds_with_threads:
                    system_invocations.append(
                        Sel4CnodeMint(
                            cnode_obj.cap_addr,
                            get_thread_sc_offset(pd.threads, maybe_child_pd.pd_id),
                            PD_CAP_BITS,
                            root_cnode_cap,
                            maybe_child_sc.cap_addr,
                            kernel_config.cap_address_bits,
                            SEL4_RIGHTS_ALL,
                            0
                        )
                    )
                    # NOTE: no need to bind SC to TCB here since this is already done for PDs with endpoints
                    # by calling Sel4TcbSetSchedParams below

                    system_invocations.append(
                        Sel4CnodeMint(
                            cnode_obj.cap_addr,
                            get_thread_cspace_offset(pd.threads, maybe_child_pd.pd_id),
                            PD_CAP_BITS,
                            root_cnode_cap,
                            maybe_child_cnode.cap_addr,
                            kernel_config.cap_address_bits,
                            SEL4_RIGHTS_ALL,
                            0
                        )
                    )

                    system_invocations.append(
                        Sel4CnodeCopy(
                            cnode_obj.cap_addr,
                            get_thread_reply_offset(pd.threads, maybe_child_pd.pd_id),
                            PD_CAP_BITS,
                            root_cnode_cap,
                            maybe_child_reply.cap_addr,
                            kernel_config.cap_address_bits,
                            SEL4_RIGHTS_ALL
                        )
                    )
        
        # Set up caps needed to support threading
        if pd in pds_with_threads:
            num_spawnable_threads = pd.threads - pd.num_child_pds

            ## Mint a endpoint for empty threads to make root PPCs, if supported, into the thread CSpace.
            ## TODO: mint a fault EP for each thread too

            if pd.accepts_root_ppc:
                root_pd_ep = pd_endpoint_objects.get(pd)
                child_cnode_cap = threads_cnodes[pd][pd.num_child_pds].cap_addr
                child_pd_badge = BADGE_TYPE_ROOT_PPC | (pd.num_child_pds) # remember that first few thread IDs are actually just PD IDs!
                invocation = Sel4CnodeMint(
                    child_cnode_cap,
                    ROOT_PD_EP_CAP_IDX,
                    EMPTY_THREAD_CAP_BITS,
                    root_cnode_cap,
                    root_pd_ep.cap_addr,
                    kernel_config.cap_address_bits,
                    SEL4_RIGHTS_ALL,
                    child_pd_badge)
                invocation.repeat(num_spawnable_threads, cnode=1, badge=1)
                system_invocations.append(invocation)

                child_tcb_cap = threads_tcbs[pd][pd.num_child_pds].cap_addr
                invocation = Sel4TcbSetCSpace(
                    child_tcb_cap,
                    child_cnode_cap,
                    kernel_config.cap_address_bits - EMPTY_THREAD_CAP_BITS
                )
                invocation.repeat(num_spawnable_threads, tcb=1, cspace_root=1)
                system_invocations.append(invocation)

            ## Mint access to empty threads in the root PD CSpace

            tcb_cap = threads_tcbs[pd][pd.num_child_pds].cap_addr
            invocation = Sel4CnodeMint(
                            cnode_obj.cap_addr,
                            BASE_TCB_CAP + pd.num_child_pds,
                            PD_CAP_BITS,
                            root_cnode_cap,
                            tcb_cap,
                            kernel_config.cap_address_bits,
                            SEL4_RIGHTS_ALL,
                            0)
            invocation.repeat(num_spawnable_threads, dest_index=1, src_obj=1)
            system_invocations.append(invocation)


            ## Mint access to scheduling contexts for threads in the root PD CSpace

            sc_cap = threads_sc[pd][pd.num_child_pds].cap_addr
            invocation = Sel4CnodeMint(
                            cnode_obj.cap_addr,
                            get_thread_sc_offset(pd.threads, pd.num_child_pds),
                            PD_CAP_BITS,
                            root_cnode_cap,
                            sc_cap,
                            kernel_config.cap_address_bits,
                            SEL4_RIGHTS_ALL,
                            0)
            invocation.repeat(num_spawnable_threads, dest_index=1, src_obj=1)
            system_invocations.append(invocation)

            invocation = Sel4SchedContextBind(sc_cap, tcb_cap)
            invocation.repeat(num_spawnable_threads, schedcontext=1, tcb=1)
            system_invocations.append(invocation)


            ## Mint access to CSpaces for threads in the root PD CSpace

            cnode_cap = threads_cnodes[pd][pd.num_child_pds].cap_addr
            invocation = Sel4CnodeMint(
                            cnode_obj.cap_addr,
                            get_thread_cspace_offset(pd.threads, pd.num_child_pds),
                            PD_CAP_BITS,
                            root_cnode_cap,
                            cnode_cap,
                            kernel_config.cap_address_bits,
                            SEL4_RIGHTS_ALL,
                            0)
            invocation.repeat(num_spawnable_threads, dest_index=1, src_obj=1)
            system_invocations.append(invocation)


            ## Copy cap to self CNode

            system_invocations.append(
                Sel4CnodeCopy(
                    cnode_obj.cap_addr,
                    SELF_CNODE_CAP_IDX,
                    PD_CAP_BITS,
                    root_cnode_cap,
                    cnode_obj.cap_addr,
                    kernel_config.cap_address_bits,
                    SEL4_RIGHTS_ALL
                )
            )


            ## Copy access to Reply caps for threads in the root PD CSpace

            reply_cap = threads_replies[pd][pd.num_child_pds].cap_addr
            invocation = Sel4CnodeCopy(
                            cnode_obj.cap_addr,
                            get_thread_reply_offset(pd.threads, pd.num_child_pds),
                            PD_CAP_BITS,
                            root_cnode_cap,
                            reply_cap,
                            kernel_config.cap_address_bits,
                            SEL4_RIGHTS_ALL)
            invocation.repeat(num_spawnable_threads, dest_index=1, src_obj=1)
            system_invocations.append(invocation)


            ## Mint access to SchedControl cap

            system_invocations.append(
                Sel4CnodeCopy(
                    cnode_obj.cap_addr,
                    SCHEDCONTROL_CAP_IDX,
                    PD_CAP_BITS,
                    root_cnode_cap,
                    kernel_boot_info.schedcontrol_cap,
                    kernel_config.cap_address_bits,
                    SEL4_RIGHTS_ALL
                )
            )


            ## Mint access to child PD VSpaces in the root PD CSpace

            # Can't repeat invocation here because VSpace caps might not be next to each other
            for pd_id, vspace in child_vspaces[pd]:
                system_invocations.append(
                    Sel4CnodeMint(
                        cnode_obj.cap_addr,
                        get_pd_vspace_offset(pd.threads, pd_id),
                        PD_CAP_BITS,
                        root_cnode_cap,
                        vspace.cap_addr,
                        kernel_config.cap_address_bits,
                        SEL4_RIGHTS_ALL,
                        0)
                )


    for cc in system.channels:
        pd_a = system.pd_by_name[cc.pd_a]
        pd_b = system.pd_by_name[cc.pd_b]
        pd_a_cnode_obj = cnode_objects_by_pd[pd_a]
        pd_b_cnode_obj = cnode_objects_by_pd[pd_b]
        pd_a_notification_obj = notification_objects_by_pd[pd_a]
        pd_b_notification_obj = notification_objects_by_pd[pd_b]

        # EPs that each PD uses to listen on (the input EP)
        pd_a_endpoint_obj = pd_endpoint_objects.get(pd_a)
        pd_b_endpoint_obj = pd_endpoint_objects.get(pd_b)

        # Set up the notification caps
        pd_a_cap_idx = BASE_OUTPUT_NTFN_CAP_IDX + cc.id_a
        pd_a_badge = 1 << cc.id_b
        assert pd_a_cap_idx < PD_CAP_SIZE
        system_invocations.append(
            Sel4CnodeMint(
                pd_a_cnode_obj.cap_addr,
                pd_a_cap_idx,
                PD_CAP_BITS,
                root_cnode_cap,
                pd_b_notification_obj.cap_addr,
                kernel_config.cap_address_bits,
                SEL4_RIGHTS_ALL, # FIXME: Check rights
                pd_a_badge)
        )

        pd_b_cap_idx = BASE_OUTPUT_NTFN_CAP_IDX + cc.id_b
        pd_b_badge = 1 << cc.id_a
        assert pd_b_cap_idx < PD_CAP_SIZE
        system_invocations.append(
            Sel4CnodeMint(
                pd_b_cnode_obj.cap_addr,
                pd_b_cap_idx,
                PD_CAP_BITS,
                root_cnode_cap,
                pd_a_notification_obj.cap_addr,
                kernel_config.cap_address_bits,
                SEL4_RIGHTS_ALL, # FIXME: Check rights
                pd_b_badge)
        )

        ## Set up the endpoint caps
        # There are 2 types of channels:
        # 1. Root PPCs, from child to root PD
        # 2. Normal PPCs between 2 normal PDs that are NOT child PDs
        # These 2 cannot exist together, and for now we say that the only endpoints
        # the child PDs get are to the root PDs and nobody else. This simplifies the
        # threading model, as each thread in a child PD will just have small CSpaces (4 slots ish)
        # and only have to communicate with their parent. Expanding the CSpaces beyond this can
        # be for future consideration.

        # A -> B
        # If PD B accepts PPCs, badge an endpoint for PD A to use.
        if pd_b.pp:
            pd_a_cap_idx = BASE_OUTPUT_EP_CAP_IDX + cc.id_a
            pd_a_badge = BADGE_TYPE_PPC | cc.id_b
            assert pd_b_endpoint_obj is not None
            assert pd_a_cap_idx < PD_CAP_SIZE
            system_invocations.append(
                Sel4CnodeMint(
                    pd_a_cnode_obj.cap_addr,
                    pd_a_cap_idx,
                    PD_CAP_BITS,
                    root_cnode_cap,
                    pd_b_endpoint_obj.cap_addr,
                    kernel_config.cap_address_bits,
                    SEL4_RIGHTS_ALL, # FIXME: Check rights
                    pd_a_badge)
            )


        # B -> A
        # If PD A accepts PPCs, badge an endpoint for PD B to use.
        if pd_a.pp:
            pd_b_cap_idx = BASE_OUTPUT_EP_CAP_IDX + cc.id_b
            pd_b_badge = BADGE_TYPE_PPC | cc.id_a
            assert pd_a_endpoint_obj is not None
            assert pd_b_cap_idx < PD_CAP_SIZE
            system_invocations.append(
                Sel4CnodeMint(
                    pd_b_cnode_obj.cap_addr,
                    pd_b_cap_idx,
                    PD_CAP_BITS,
                    root_cnode_cap,
                    pd_a_endpoint_obj.cap_addr,
                    kernel_config.cap_address_bits,
                    SEL4_RIGHTS_ALL, # FIXME: Check rights
                    pd_b_badge)
            )


    # All minting is complete at this point

    # Associate badges
    # FIXME: This could use repeat
    for notification_obj, pd in zip(notification_objects, system.protection_domains):
        for irq_cap_address, badged_notification_cap_address in zip(irq_cap_addresses[pd], badged_irq_caps[pd]):
            system_invocations.append(Sel4IrqHandlerSetNotification(irq_cap_address, badged_notification_cap_address))


    # Initialise the VSpaces -- assign them all the the initial asid pool.
    for map_cls, descriptors, objects in [
        (Sel4PageUpperDirectoryMap, uds, ud_objects),
        (Sel4PageDirectoryMap, ds, d_objects),
        (Sel4PageTableMap, pts, pt_objects),
    ]:
        for ((pd_idx, vaddr), obj) in zip(descriptors, objects):
            vspace_obj = vspace_objects[pd_idx]
            system_invocations.append(
                map_cls(
                    obj.cap_addr,
                    vspace_obj.cap_addr,
                    vaddr,
                    SEL4_ARM_DEFAULT_VMATTRIBUTES
                )
            )

    # Now maps all the pages
    for page_cap_address, pd_idx, vaddr, rights, attrs, count, vaddr_incr in page_descriptors:
        vspace_obj = vspace_objects[pd_idx]
        invocation = Sel4PageMap(page_cap_address, vspace_obj.cap_addr, vaddr, rights, attrs)
        invocation.repeat(count, page=1, vaddr=vaddr_incr)
        system_invocations.append(invocation)

    # Initialise the TCBs
    #
    # set scheduling parameters (SetSchedParams)
    for idx, (pd, schedcontext_obj, tcb_obj) in enumerate(zip(system.protection_domains, schedcontext_objects, tcb_objects)):
        system_invocations.append(
            Sel4SchedControlConfigureFlags(
                kernel_boot_info.schedcontrol_cap,
                schedcontext_obj.cap_addr,
                pd.budget,
                pd.period,
                0,
                0, # wtf is this "badge" even used for? idk, the actual badge is the fault ep
                0 # @andyb: default to periodic tasks for now until we find a better way to manage this (xml param?).
            )
        )

        # For now only child PDs can have timeout faults, TODO @andyb
        if pd.parent and pd.parent.handle_timeouts:
            system_invocations.append(
                Sel4TcbSetTimeoutEndpoint(
                    tcb_obj.cap_addr,
                    badged_fault_ep + idx
                )
            )
    
    # TODO: @andyb merge this up?
    # once we set the sched params for the SC objects, bind them to TCBs.
    for tcb_obj, schedcontext_obj, pd in zip(tcb_objects, schedcontext_objects, system.protection_domains):
        # Monitor no longer receives faults, set fault_ep to 0
        system_invocations.append(Sel4TcbSetSchedParams(tcb_obj.cap_addr, INIT_TCB_CAP_ADDRESS, pd.priority, pd.priority, schedcontext_obj.cap_addr, 0))

    # set vspace / cspace (SetSpace)
    # FIXME: badged endpoints for faulting -> root PDs eventually go here. For now set to 0.
    # invocation = Sel4TcbSetSpace(tcb_objects[0].cap_addr, badged_fault_ep, cnode_objects[0].cap_addr, kernel_config.cap_address_bits - PD_CAP_BITS, vspace_objects[0].cap_addr, 0)
    # TODO: this badged_fault_ep should just be a temporary one that gets destructed on the way out of the sysinit thread
    invocation = Sel4TcbSetSpace(tcb_objects[0].cap_addr, badged_fault_ep, cnode_objects[0].cap_addr, kernel_config.cap_address_bits - PD_CAP_BITS, vspace_objects[0].cap_addr, 0)
    invocation.repeat(len(system.protection_domains), tcb=1, fault_ep=1, cspace_root=1, vspace_root=1)
    system_invocations.append(invocation)

    # Set IPC buffers for each PD
    for tcb_obj, pd in zip(tcb_objects, system.protection_domains):
        cap_addr, ipc_buffer_vaddr_base = ipc_buffers_by_pd[pd]
        invocation = Sel4TcbSetIpcBuffer(
            tcb_obj.cap_addr,
            ipc_buffer_vaddr_base,
            cap_addr
        )
        num_ipc_buffers = pd.parent.threads if pd.parent and pd.parent.threads > 0 else 1
        invocation.repeat(num_ipc_buffers, buffer=ProtectionDomainVSpace.VSPACE_IPCBUFF_SIZE, buffer_frame=1)
        system_invocations.append(invocation)

    # set register (entry point)
    for tcb_obj, pd in zip(tcb_objects, system.protection_domains):
        system_invocations.append(
            Sel4TcbWriteRegisters(
                tcb_obj.cap_addr,
                False,
                0, # no flags on ARM
                Sel4Aarch64Regs(
                    pc=pd_elf_files[pd].entry,
                    x0=pd.pd_id,
                    x1=1 # Root thread?
                )
            )
        )
    # bind the notification object
    invocation = Sel4TcbBindNotification(tcb_objects[0].cap_addr, notification_objects[0].cap_addr)
    invocation.repeat(count=len(system.protection_domains), tcb=1, notification=1)
    system_invocations.append(invocation)

    # set domain

    # suppose 3 partitions, we currently know that sysxml will layout system.protection_domains such that
    # the list will be as such: XXXXXXXYYYYZZZZZZZZZZZ where X, Y, Z are PDs belonging to their respective
    # partitions. so here we just repeat for the size of each partition
    start_tcb_of_partition = tcb_objects[0].cap_addr
    for partition_id, partition in enumerate(system.partitions):
        invocation = Sel4DomainSet(DOMAIN_CAP_ADDRESS, partition_id, start_tcb_of_partition)
        invocation.repeat(count=len(partition.protection_domains), tcb=1)
        system_invocations.append(invocation)
        start_tcb_of_partition += len(partition.protection_domains)

    # Resume (start) all the threads
    invocation = Sel4TcbResume(tcb_objects[0].cap_addr)
    invocation.repeat(count=len(system.protection_domains), tcb=1)
    system_invocations.append(invocation)

    # Suspend self (Sysinit)
    # TODO: @andyb: cleanup before we suspend
    system_invocations.append(Sel4TcbSuspend(INIT_TCB_CAP_ADDRESS))

    # All of the objects are created at this point; we don't need to both
    # the allocators from here.

    # And now we are done. We have all the invocations

    system_invocation_data = b''
    for system_invocation in system_invocations:
        system_invocation_data += system_invocation._get_raw_invocation()


    for pd in system.protection_domains:
        # Could use pd.elf_file.write_symbol here to update variables if required.
        pd_elf_files[pd].write_symbol("sel4cp_name", pack("<16s", pd.name.encode("utf8")))
        pd_elf_files[pd].write_symbol("passive", pack("?", pd.passive))


    for pd in system.protection_domains:
        for setvar in pd.setvars:
            if setvar.region_paddr is not None:
                for mr in system.memory_regions:
                    if mr.name == setvar.region_paddr:
                        break
                else:
                    raise Exception(f"can't find region: {setvar.region_paddr}")
                value = mr_pages[mr][0].phys_addr
            elif setvar.vaddr is not None:
                value = setvar.vaddr
            try:
                pd_elf_files[pd].write_symbol(setvar.symbol, pack("<Q", value))
            except KeyError:
                raise Exception(f"Unable to patch variable '{setvar.symbol}' in protection domain: '{pd.name}': variable not found.")

    return BuiltSystem(
        number_of_system_caps = final_cap_slot, #init_system._cap_slot,
        invocation_data_size = len(system_invocation_data),
        bootstrap_invocations = bootstrap_invocations,
        system_invocations = system_invocations,
        kernel_boot_info = kernel_boot_info,
        reserved_region = reserved_region,
        cap_lookup = cap_address_names,
        tcb_caps = tcb_caps,
        sched_caps = schedcontext_caps,
        ntfn_caps = notification_caps,
        regions = regions,
        kernel_objects = init_system._objects,
        initial_task_phys_region = initial_task_phys_region,
        initial_task_virt_region = initial_task_virt_region,
    )


def main() -> int:
    if "SEL4CP_SDK" in environ:
        SDK_DIR = Path(environ["SEL4CP_SDK"])
    elif getattr(sys, 'oxidized', False):
        # If we a compiled binary we know where the root is
        SDK_DIR = Path(executable).parent.parent
    else:
        print("Error: SEL4CP_SDK must be set")
        return 1
    assert SDK_DIR is not None
    if not SDK_DIR.exists():
        print(f"Error: SDK directory '{SDK_DIR}' does not exist. Check SEL4CP_SDK environment variable is set correctly")
        return 1

    boards_path = SDK_DIR / "board"
    if not boards_path.exists():
        print(f"Error: SDK  directory '{SDK_DIR}' does not have a 'board' sub-directory. Check SEL4CP_SDK environment variable is set correctly")
        return 1

    available_boards = [p.name for p in boards_path.iterdir() if p.is_dir()]

    parser = ArgumentParser()
    parser.add_argument("system", type=Path)
    parser.add_argument("-o", "--output", type=Path, default=Path("loader.img"))
    parser.add_argument("-r", "--report", type=Path, default=Path("report.txt"))
    parser.add_argument("--board", required=True, choices=available_boards)
    parser.add_argument("--config", required=True)
    parser.add_argument("--search-path", nargs='*', type=Path)
    args = parser.parse_args()

    board_path = boards_path / args.board
    if not board_path.exists():
        print(f"Error: board path '{board_path}' doesn't exist.")
        return 1

    available_configs = [p.name for p in board_path.iterdir() if p.is_dir() and p.name != "example"]
    if args.config not in available_configs:
        parser.error(f"argument --config: invalid choice: '{args.config}' (choose from {available_configs})")

    elf_path = SDK_DIR / "board" / args.board / args.config / "elf"
    loader_elf_path = elf_path / "loader.elf"
    kernel_elf_path = elf_path / "sel4.elf"
    sysinit_elf_path = elf_path / "sysinit.elf"

    if not elf_path.exists():
        print(f"Error: board ELF directory '{elf_path}' does not exist")
        return 1
    if not loader_elf_path.exists():
        print(f"Error: loader ELF '{loader_elf_path}' does not exist")
        return 1
    if not kernel_elf_path.exists():
        print(f"Error: loader ELF '{kernel_elf_path}' does not exist")
        return 1
    if not sysinit_elf_path.exists():
        print(f"Error: sysinit ELF '{sysinit_elf_path}' does not exist")
        return 1

    if not args.system.exists():
        print(f"Error: system description file '{args.system}' does not exist")
        return 1

    search_paths = [] if args.search_path is None else args.search_path
    search_paths.insert(0, Path.cwd())

    system_description = xml2system(args.system, default_platform_description)

    kernel_elf = ElfFile.from_path(kernel_elf_path)

    # FIXME: The kernel config should be an output of the kernel
    # build step (or embedded into the kernel elf file in some manner
    kernel_config = KernelConfig(
        word_size = 64,
        minimum_page_size = kb(4),
        paddr_user_device_top = (1 << 40),
        kernel_frame_size = (1 << 12),
        init_cnode_bits = 12,
        cap_address_bits=64,
        fan_out_limit=256
    )

    sysinit_elf = ElfFile.from_path(sysinit_elf_path)
    if len(sysinit_elf.segments) > 1:
        raise Exception("sysinit ({sysinit_elf_path}) has {len(sysinit_elf.segments)} segments; must only have one")

    invocation_table_size = kernel_config.minimum_page_size
    system_cnode_size = 2

    while True:
        built_system = build_system(
            kernel_config,
            kernel_elf,
            sysinit_elf,
            system_description,
            invocation_table_size,
            system_cnode_size,
            search_paths,
        )
        print(f"BUILT: {system_cnode_size=} {built_system.number_of_system_caps=} {invocation_table_size=} {built_system.invocation_data_size=}")
        if (built_system.number_of_system_caps <= system_cnode_size and
            built_system.invocation_data_size <= invocation_table_size):
            break

        # Recalculate the sizes for the next iteration
        new_invocation_table_size = round_up(built_system.invocation_data_size, kernel_config.minimum_page_size)
        new_system_cnode_size = 2 ** int(ceil(log2(built_system.number_of_system_caps)))

        invocation_table_size = max(invocation_table_size, new_invocation_table_size)
        system_cnode_size = max(system_cnode_size, new_system_cnode_size)

    # At this point we just need to patch the files (in memory) and write out the final image.

    # A: The system initializer

    # A.1: As part of emulated boot we determined exactly how the kernel would
    # create untyped objects. Through testing we know that this matches, but
    # we could have a bug, or the kernel could change. It that happens we are
    # in a bad spot! Things will break. So we write out this information so that
    # the sysinit task can double check this at run time.
    _, untyped_info_size = sysinit_elf.find_symbol(SYSINIT_CONFIG.untyped_info_symbol_name)
    max_untyped_objects = SYSINIT_CONFIG.max_untyped_objects(untyped_info_size)
    if len(built_system.kernel_boot_info.untyped_objects) > max_untyped_objects:
        raise Exception(f"Too many untyped objects: monitor ({sysinit_elf_path}) supports {max_untyped_objects:,d} regions. System has {len(built_system.kernel_boot_info.untyped_objects):,d} objects.")
    
    untyped_info_header = SYSINIT_CONFIG.untyped_info_header_struct.pack(
            built_system.kernel_boot_info.untyped_objects[0].cap,
            built_system.kernel_boot_info.untyped_objects[-1].cap + 1
        )
    untyped_info_object_data = []
    for idx, ut in enumerate(built_system.kernel_boot_info.untyped_objects):
        object_data = SYSINIT_CONFIG.untyped_info_object_struct.pack(ut.base, ut.size_bits, ut.is_device)
        untyped_info_object_data.append(object_data)

    untyped_info_data = untyped_info_header + b''.join(untyped_info_object_data)
    sysinit_elf.write_symbol(SYSINIT_CONFIG.untyped_info_symbol_name, untyped_info_data)

    _, bootstrap_invocation_data_size = sysinit_elf.find_symbol(SYSINIT_CONFIG.bootstrap_invocation_data_symbol_name)

    bootstrap_invocation_data = b''
    for bootstrap_invocation in built_system.bootstrap_invocations:
        bootstrap_invocation_data += bootstrap_invocation._get_raw_invocation()

    if len(bootstrap_invocation_data) > bootstrap_invocation_data_size:
        print("INTERNAL ERROR: bootstrap invocations too large", file=stderr)
        print(f"bootstrap invocation array size   : {bootstrap_invocation_data_size:d}", file=stderr)
        print(f"bootstrap invocation required size: {len(bootstrap_invocation_data):d}", file=stderr)
        for bootstrap_invocation in built_system.bootstrap_invocations:
            print(invocation_to_str(bootstrap_invocation, built_system.cap_lookup), file=stderr)

        raise UserError("bootstrap invocations too large for system initializer")

    sysinit_elf.write_symbol(SYSINIT_CONFIG.bootstrap_invocation_count_symbol_name, pack("<Q", len(built_system.bootstrap_invocations)))
    sysinit_elf.write_symbol(SYSINIT_CONFIG.system_invocation_count_symbol_name, pack("<Q", len(built_system.system_invocations)))
    sysinit_elf.write_symbol(SYSINIT_CONFIG.bootstrap_invocation_data_symbol_name, bootstrap_invocation_data)

    system_invocation_data = b''
    for system_invocation in built_system.system_invocations:
        system_invocation_data += system_invocation._get_raw_invocation()

    regions: List[Tuple[int, Union[bytes, bytearray]]] = [(built_system.reserved_region.base, system_invocation_data)]
    regions += [(r.addr, r.data) for r in built_system.regions]

    tcb_caps = built_system.tcb_caps
    sched_caps = built_system.sched_caps
    ntfn_caps = built_system.ntfn_caps

    # FIXME: @andyb sysinit task does not need these symbols, only the monitor
    # monitor_elf.write_symbol("fault_ep", pack("<Q", built_system.fault_ep_cap_address))
    # monitor_elf.write_symbol("reply", pack("<Q", built_system.reply_cap_address))
    # monitor_elf.write_symbol("tcbs", pack("<Q" + "Q" * len(tcb_caps), 0, *tcb_caps))
    # monitor_elf.write_symbol("scheduling_contexts", pack("<Q" + "Q" * len(sched_caps), 0, *sched_caps))
    # monitor_elf.write_symbol("notification_caps", pack("<Q" + "Q" * len(ntfn_caps), 0, *ntfn_caps))
    # names_array = bytearray([0] * (64 * 16))
    # for idx, pd in enumerate(system_description.protection_domains, 1):
    #     nm = pd.name.encode("utf8")[:15]
    #     names_array[idx * 16:idx * 16+len(nm)] = nm
    # monitor_elf.write_symbol("pd_names", names_array)


    # B: The loader

    # B.1: The loader is primarily about loading 'regions' of memory.
    # so here we determine which regions it should be loading into
    # physical memory
    cap_lookup = built_system.cap_lookup

    # Reporting
    with args.report.open("w") as f:
        f.write("# Kernel Boot Info\n\n")
        f.write(f"    # of fixed caps     : {built_system.kernel_boot_info.fixed_cap_count:8,d}\n")
        f.write(f"    # of page table caps: {built_system.kernel_boot_info.paging_cap_count:8,d}\n")
        f.write(f"    # of page caps      : {built_system.kernel_boot_info.page_cap_count:8,d}\n")
        f.write(f"    # of untyped objects: {len(built_system.kernel_boot_info.untyped_objects):8,d}\n")
        f.write("\n")
        f.write("# Loader Regions\n\n")
        for region in built_system.regions:
            f.write(f"       {region}\n")
        f.write("\n")
        f.write("# System Initializer (Initial Task) Info\n\n")
        f.write(f"     virtual memory : {built_system.initial_task_virt_region}\n")
        f.write(f"     physical memory: {built_system.initial_task_phys_region}\n")
        f.write("\n")
        f.write("# Allocated Kernel Objects Summary\n\n")
        f.write(f"     # of allocated objects: {len(built_system.kernel_objects):,d}\n")
        f.write("\n")
        f.write("# Bootstrap Kernel Invocations Summary\n\n")
        f.write(f"     # of invocations   : {len(built_system.bootstrap_invocations):10,d}\n")
        f.write(f"     size of invocations: {len(bootstrap_invocation_data):10,d}\n")
        f.write("\n")
        f.write("# System Kernel Invocations Summary\n\n")
        f.write(f"     # of invocations   : {len(built_system.system_invocations):10,d}\n")
        f.write(f"     size of invocations: {len(system_invocation_data):10,d}\n")
        f.write("\n")
        f.write("# Allocated Kernel Objects Detail\n\n")
        for ko in built_system.kernel_objects:
            f.write(f"    {ko.name:50s} {ko.object_type} cap_addr={ko.cap_addr:x} phys_addr={ko.phys_addr:x}\n")
        f.write("\n")
        f.write("# Bootstrap Kernel Invocations Detail\n\n")
        for idx, invocation in enumerate(built_system.bootstrap_invocations):
            f.write(f"    0x{idx:04x} {invocation_to_str(invocation, cap_lookup)}\n")
        f.write("\n")
        f.write("# System Kernel Invocations Detail\n\n")
        for idx, invocation in enumerate(built_system.system_invocations):
            f.write(f"    0x{idx:04x} {invocation_to_str(invocation, cap_lookup)}\n")

    # FIXME: Verify that the regions do not overlap!
    loader = Loader(
        loader_elf_path,
        kernel_elf,
        sysinit_elf,
        built_system.initial_task_phys_region.base,
        built_system.reserved_region,
        regions,
    )
    loader.write_image(args.output)

    return 0


if __name__ == "__main__":
    try:
        exit(main())
    except UserError as e:
        print(e)
        exit(1)
